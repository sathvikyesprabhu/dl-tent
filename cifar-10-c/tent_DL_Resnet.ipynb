{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from networks import ResNet\n",
    "import tent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "1\n",
      "NVIDIA GeForce GTX 1070\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "(50000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "mean = (0.4914, 0.4822, 0.4465)\n",
    "std = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "transform_train = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.RandomCrop(32, padding=4),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean, std)\n",
    "                                    ])\n",
    "transform_test = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean, std)])\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "valset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "print(trainset.train_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.175818 M parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9563, -0.5725, -0.0706,  0.1222,  0.2747,  0.1813,  0.0520,  0.7712,\n",
       "          0.3720,  0.3805],\n",
       "        [ 1.0430, -0.4286, -0.1176,  0.0610,  0.2832,  0.2366, -0.0171,  0.7024,\n",
       "          0.4616,  0.4219]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = models.resnet18()\n",
    "# model.fc = nn.Linear(in_features=512, out_features=10, bias=True)\n",
    "\n",
    "model = ResNet(18,10)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters())/1000000, \"M parameters\")\n",
    "x = torch.randn(2,3,32,32)\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() # CE Loss, Takes care of applying softmax\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) # Adam optimizer\n",
    "# optimizer = torch.optim.SGD(model.parameters(),lr=0.1)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 | Train loss: 1.4659 | Test loss: 1.4004 | Train acc.: 0.4623 | Test acc.: 0.5047\n",
      "\n",
      "Epoch:1 | Train loss: 1.0459 | Test loss: 1.2310 | Train acc.: 0.6227 | Test acc.: 0.5792\n",
      "\n",
      "Epoch:2 | Train loss: 0.8942 | Test loss: 0.9869 | Train acc.: 0.6829 | Test acc.: 0.6645\n",
      "\n",
      "Epoch:3 | Train loss: 0.7806 | Test loss: 0.8679 | Train acc.: 0.7259 | Test acc.: 0.6926\n",
      "\n",
      "Epoch:4 | Train loss: 0.7075 | Test loss: 0.7833 | Train acc.: 0.7527 | Test acc.: 0.7374\n",
      "\n",
      "Epoch:5 | Train loss: 0.6524 | Test loss: 0.8065 | Train acc.: 0.7725 | Test acc.: 0.7390\n",
      "\n",
      "Epoch:6 | Train loss: 0.6180 | Test loss: 0.6557 | Train acc.: 0.7858 | Test acc.: 0.7697\n",
      "\n",
      "Epoch:7 | Train loss: 0.5789 | Test loss: 0.6989 | Train acc.: 0.7988 | Test acc.: 0.7671\n",
      "\n",
      "Epoch:8 | Train loss: 0.5499 | Test loss: 0.6726 | Train acc.: 0.8086 | Test acc.: 0.7720\n",
      "\n",
      "Epoch:9 | Train loss: 0.5252 | Test loss: 0.5660 | Train acc.: 0.8195 | Test acc.: 0.8092\n",
      "\n",
      "Epoch:10 | Train loss: 0.5081 | Test loss: 0.6350 | Train acc.: 0.8223 | Test acc.: 0.7911\n",
      "\n",
      "Epoch:11 | Train loss: 0.4839 | Test loss: 0.5691 | Train acc.: 0.8321 | Test acc.: 0.8079\n",
      "\n",
      "Epoch:12 | Train loss: 0.4662 | Test loss: 0.6109 | Train acc.: 0.8373 | Test acc.: 0.7990\n",
      "\n",
      "Epoch:13 | Train loss: 0.4476 | Test loss: 0.5847 | Train acc.: 0.8456 | Test acc.: 0.8022\n",
      "\n",
      "Epoch:14 | Train loss: 0.4378 | Test loss: 0.5741 | Train acc.: 0.8474 | Test acc.: 0.8102\n",
      "\n",
      "Epoch:15 | Train loss: 0.4224 | Test loss: 0.4867 | Train acc.: 0.8526 | Test acc.: 0.8382\n",
      "\n",
      "Epoch:16 | Train loss: 0.4106 | Test loss: 0.6157 | Train acc.: 0.8579 | Test acc.: 0.7971\n",
      "\n",
      "Epoch:17 | Train loss: 0.3996 | Test loss: 0.5194 | Train acc.: 0.8602 | Test acc.: 0.8343\n",
      "\n",
      "Epoch:18 | Train loss: 0.3891 | Test loss: 0.4805 | Train acc.: 0.8641 | Test acc.: 0.8411\n",
      "\n",
      "Epoch:19 | Train loss: 0.3813 | Test loss: 0.5180 | Train acc.: 0.8671 | Test acc.: 0.8334\n",
      "\n",
      "Epoch:20 | Train loss: 0.3667 | Test loss: 0.4829 | Train acc.: 0.8720 | Test acc.: 0.8393\n",
      "\n",
      "Epoch:21 | Train loss: 0.3632 | Test loss: 0.4944 | Train acc.: 0.8734 | Test acc.: 0.8367\n",
      "\n",
      "Epoch:22 | Train loss: 0.3481 | Test loss: 0.4468 | Train acc.: 0.8799 | Test acc.: 0.8527\n",
      "\n",
      "Epoch:23 | Train loss: 0.3444 | Test loss: 0.5326 | Train acc.: 0.8791 | Test acc.: 0.8305\n",
      "\n",
      "Epoch:24 | Train loss: 0.3360 | Test loss: 0.4735 | Train acc.: 0.8817 | Test acc.: 0.8495\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(25):  # num epochs\n",
    "    \n",
    "    # Training\n",
    "    model.train() # Set to train mode\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for i, data in enumerate(trainloader): # Get data batch-wise\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # print(inputs.shape, inputs.dtype)\n",
    "        # labels = labels.type(torch.LongTensor)\n",
    "\n",
    "        # zero out gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs) # forward pass\n",
    "        # print(outputs.shape, labels.shape)\n",
    "        loss = criterion(outputs, labels) # Get loss\n",
    "        loss.backward() # Backward pass\n",
    "        optimizer.step() # Optimize model weights\n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # Get predictions\n",
    "        running_loss += loss.detach() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    # Get loss and accuracy\n",
    "    train_loss = (running_loss / len(trainset))\n",
    "    train_loss_list.append(train_loss.item())\n",
    "    train_accuracy = (running_corrects.float() / len(trainset))\n",
    "\n",
    "    # Testing\n",
    "    model.eval() # Set to eval mode\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    y = []; yhat = []\n",
    "\n",
    "    for i, data in enumerate(valloader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # labels = labels.type(torch.LongTensor)\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        with torch.no_grad(): # Don't build computation graph for testing\n",
    "            outputs = model(inputs)\n",
    "        # print(outputs.shape, labels.shape)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        y.append(labels.tolist())\n",
    "        yhat.append(preds.tolist())\n",
    "        running_loss += loss.detach() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "    \n",
    "    # Get loss and accuracy\n",
    "    test_loss = (running_loss / len(valset))\n",
    "    test_loss_list.append(test_loss.item())\n",
    "    test_accuracy = (running_corrects.float() / len(valset))\n",
    "\n",
    "    acc = test_accuracy.item()\n",
    "    if(acc > best_acc):\n",
    "        best_acc = acc\n",
    "        torch.save(model.state_dict(),\"checkpoint/resnet18.pth\")\n",
    "\n",
    "    # Display loss, accuracy values after each epoch\n",
    "    print(\"Epoch:{} | Train loss: {:.4f} | Test loss: {:.4f} | Train acc.: {:.4f} | Test acc.: {:.4f}\\n\"\n",
    "              .format(epoch, train_loss.item(),test_loss.item(),train_accuracy.item(),test_accuracy.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet(18,10).to(device)\n",
    "model.load_state_dict(torch.load(\"checkpoint/resnet18.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.4468 | Test acc.: 0.8527\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "model.eval() # Set to eval mode\n",
    "running_loss = 0.0\n",
    "running_corrects = 0\n",
    "y = []; yhat = []\n",
    "\n",
    "for i, data in enumerate(valloader):\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    # labels = labels.type(torch.LongTensor)\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    with torch.no_grad(): # Don't build computation graph for testing\n",
    "        outputs = model(inputs)\n",
    "    # print(outputs.shape, labels.shape)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    y.append(labels.tolist())\n",
    "    yhat.append(preds.tolist())\n",
    "    running_loss += loss.detach() * inputs.size(0)\n",
    "    running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "# Get loss and accuracy\n",
    "test_loss = (running_loss / len(valset))\n",
    "test_loss_list.append(test_loss.item())\n",
    "test_accuracy = (running_corrects.float() / len(valset))\n",
    "\n",
    "# Display loss, accuracy values after each epoch\n",
    "print(\"Test loss: {:.4f} | Test acc.: {:.4f}\\n\"\n",
    "            .format(test_loss.item(),test_accuracy.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TENT\n",
    "\n",
    "model = tent.configure_model(model)\n",
    "params, param_names = tent.collect_params(model)\n",
    "optimizer = torch.optim.Adam(params, lr=1e-3)\n",
    "tented_model = tent.Tent(model, optimizer)\n",
    "\n",
    "tented_model = tented_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.4853 | Test acc.: 0.8375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Testing\n",
    "tented_model.eval() # Set to eval mode\n",
    "running_loss = 0.0\n",
    "running_corrects = 0\n",
    "y = []; yhat = []\n",
    "\n",
    "for i, data in enumerate(valloader):\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    # labels = labels.type(torch.LongTensor)\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    with torch.no_grad(): # Don't build computation graph for testing\n",
    "        outputs = tented_model(inputs)\n",
    "    # print(outputs.shape, labels.shape)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    y.append(labels.tolist())\n",
    "    yhat.append(preds.tolist())\n",
    "    running_loss += loss.detach() * inputs.size(0)\n",
    "    running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "# Get loss and accuracy\n",
    "test_loss = (running_loss / len(valset))\n",
    "test_loss_list.append(test_loss.item())\n",
    "test_accuracy = (running_corrects.float() / len(valset))\n",
    "\n",
    "# Display loss, accuracy values after each epoch\n",
    "print(\"Test loss: {:.4f} | Test acc.: {:.4f}\\n\"\n",
    "            .format(test_loss.item(),test_accuracy.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR10-C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average: 0.8236   brightness.npy\n",
      "Average: 0.5928   contrast.npy\n",
      "Average: 0.6924   defocus_blur.npy\n",
      "Average: 0.7049   elastic_transform.npy\n",
      "Average: 0.7488   fog.npy\n",
      "Average: 0.6417   frost.npy\n",
      "Average: 0.6115   gaussian_blur.npy\n",
      "Average: 0.4521   gaussian_noise.npy\n",
      "Average: 0.421   glass_blur.npy\n",
      "Average: 0.5461   impulse_noise.npy\n",
      "Average: 0.7211   jpeg_compression.npy\n",
      "Average: 0.6134   motion_blur.npy\n",
      "Average: 0.6485   pixelate.npy\n",
      "Average: 0.8054   saturate.npy\n",
      "Average: 0.5312   shot_noise.npy\n",
      "Average: 0.6456   snow.npy\n",
      "Average: 0.7402   spatter.npy\n",
      "Average: 0.5492   speckle_noise.npy\n",
      "Average: 0.6276   zoom_blur.npy\n",
      "Mean:  0.6377515629718178\n"
     ]
    }
   ],
   "source": [
    "model = ResNet(18,10).to(device)\n",
    "model.load_state_dict(torch.load(\"checkpoint/resnet18.pth\"))\n",
    "\n",
    "chalPath = 'data/CIFAR-10-C/'\n",
    "chals = sorted(os.listdir(chalPath))\n",
    "\n",
    "chal_labels = valset.test_labels\n",
    "chal_labels = torch.Tensor(chal_labels)\n",
    "chal_labels = chal_labels.long()\n",
    "\n",
    "def preprocess_test(X):\n",
    "\n",
    "    N, H, W, C = X.shape\n",
    "    Y = torch.zeros(N, C, H, W)\n",
    "    mean = (0.4914, 0.4822, 0.4465)\n",
    "    std = (0.2023, 0.1994, 0.2010)  \n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean,std),\n",
    "    ])\n",
    "\n",
    "    for n in range(len(X)):\n",
    "        Y[n] =  transform_test(X[n])\n",
    "\n",
    "    return Y\n",
    "\n",
    "model.eval()\n",
    "avg_list = []\n",
    "\n",
    "for challenge in range(len(chals)):\n",
    "    chal_data = np.load(chalPath + chals[challenge])\n",
    "    # chal_data = np.transpose(chal_data, (0,3,1,2))\n",
    "\n",
    "    avg = 0\n",
    "    for j in range(5):\n",
    "        chal_temp_data = chal_data[j * 10000:(j + 1) * 10000]\n",
    "        chal_temp_data = preprocess_test(chal_temp_data)\n",
    "\n",
    "        chal_dataset = torch.utils.data.TensorDataset(chal_temp_data, chal_labels)\n",
    "        chal_loader = torch.utils.data.DataLoader(chal_dataset, batch_size=128)\n",
    "        chal_error = 0\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in chal_loader:\n",
    "                # cost, err, probs = net.sample_eval(x, y, Nsamples=10, logits=False)\n",
    "                # preds_list.append(probs.cpu().numpy())\n",
    "                # chal_error += err.cpu().numpy()\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                outputs = model(x)\n",
    "                correct += torch.sum(torch.argmax(outputs,1) == y)\n",
    "            # print(err)\n",
    "\n",
    "        # print(chal_error)\n",
    "        chal_acc = (correct/len(chal_dataset)).item()\n",
    "        avg += chal_acc\n",
    "        # print(round(chal_acc,4))\n",
    "    \n",
    "    avg /= 5\n",
    "    avg_list.append(avg)\n",
    "    print(\"Average:\", round(avg,4),\" \", chals[challenge])\n",
    "\n",
    "print(\"Mean: \", np.mean(avg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"resnet_base.npy\",avg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average: 0.8257   brightness.npy\n",
      "Average: 0.768   contrast.npy\n",
      "Average: 0.8098   defocus_blur.npy\n",
      "Average: 0.7573   elastic_transform.npy\n",
      "Average: 0.7965   fog.npy\n",
      "Average: 0.7331   frost.npy\n",
      "Average: 0.7973   gaussian_blur.npy\n",
      "Average: 0.6862   gaussian_noise.npy\n",
      "Average: 0.6061   glass_blur.npy\n",
      "Average: 0.6745   impulse_noise.npy\n",
      "Average: 0.737   jpeg_compression.npy\n",
      "Average: 0.768   motion_blur.npy\n",
      "Average: 0.769   pixelate.npy\n",
      "Average: 0.8161   saturate.npy\n",
      "Average: 0.7181   shot_noise.npy\n",
      "Average: 0.6984   snow.npy\n",
      "Average: 0.7693   spatter.npy\n",
      "Average: 0.7128   speckle_noise.npy\n",
      "Average: 0.7944   zoom_blur.npy\n",
      "Mean:  0.7493420883228904\n"
     ]
    }
   ],
   "source": [
    "# Tented\n",
    "\n",
    "chalPath = 'data/CIFAR-10-C/'\n",
    "chals = sorted(os.listdir(chalPath))\n",
    "\n",
    "chal_labels = valset.test_labels\n",
    "chal_labels = torch.Tensor(chal_labels)\n",
    "chal_labels = chal_labels.long()\n",
    "\n",
    "def preprocess_test(X):\n",
    "\n",
    "    N, H, W, C = X.shape\n",
    "    Y = torch.zeros(N, C, H, W)\n",
    "    mean = (0.4914, 0.4822, 0.4465)\n",
    "    std = (0.2023, 0.1994, 0.2010)  \n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean,std),\n",
    "    ])\n",
    "\n",
    "    for n in range(len(X)):\n",
    "        Y[n] =  transform_test(X[n])\n",
    "\n",
    "    return Y\n",
    "\n",
    "avg_list = []\n",
    "\n",
    "for challenge in range(len(chals)):\n",
    "    chal_data = np.load(chalPath + chals[challenge])\n",
    "    # chal_data = np.transpose(chal_data, (0,3,1,2))\n",
    "\n",
    "    avg = 0\n",
    "    for j in range(5):\n",
    "\n",
    "        # Load tented model\n",
    "        model = ResNet(18,10)\n",
    "        model.load_state_dict(torch.load(\"checkpoint/resnet18.pth\"))\n",
    "        \n",
    "        net = tent.configure_model(model)\n",
    "        params, param_names = tent.collect_params(net)\n",
    "        optimizer = torch.optim.Adam(params, lr=1e-3)\n",
    "        tented_model = tent.Tent(net, optimizer)\n",
    "\n",
    "        tented_model = tented_model.to(device)\n",
    "\n",
    "        \n",
    "        chal_temp_data = chal_data[j * 10000:(j + 1) * 10000]\n",
    "        chal_temp_data = preprocess_test(chal_temp_data)\n",
    "\n",
    "        chal_dataset = torch.utils.data.TensorDataset(chal_temp_data, chal_labels)\n",
    "        chal_loader = torch.utils.data.DataLoader(chal_dataset, batch_size=128)\n",
    "        chal_error = 0\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        tented_model.eval()\n",
    "        \n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in chal_loader:\n",
    "                # cost, err, probs = net.sample_eval(x, y, Nsamples=10, logits=False)\n",
    "                # preds_list.append(probs.cpu().numpy())\n",
    "                # chal_error += err.cpu().numpy()\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                outputs = tented_model(x)\n",
    "                correct += torch.sum(torch.argmax(outputs,1) == y)\n",
    "            # print(err)\n",
    "\n",
    "        # print(chal_error)\n",
    "        chal_acc = (correct/len(chal_dataset)).item()\n",
    "        avg += chal_acc\n",
    "        # print(round(chal_acc,4))\n",
    "    \n",
    "    avg /= 5\n",
    "    avg_list.append(avg)\n",
    "    print(\"Average:\", round(avg,4),\" \", chals[challenge])\n",
    "\n",
    "print(\"Mean: \", np.mean(avg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"resnet_tent.npy\",avg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = np.load(\"resnet_base.npy\")\n",
    "list2 = np.load(\"resnet_tent.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtQAAAGDCAYAAAALTociAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3XmYHVWZ+PHvm41d1gAxAYOyCQQihACKLLKDEBlmlGXUiP4YHILoDA64EsRxmwXHAckwyKYsosMqQQQRUBZNgmHfIgYIaxL2KCSdvL8/qjpcmu7kJpW6tzv5fp6nntRyqs5763Zuv33uqXMiM5EkSZK0dPq1OwBJkiSpLzOhliRJkiowoZYkSZIqMKGWJEmSKjChliRJkiowoZYkSZIqMKGWJNUiIsZHxE/K9Y0j4rWI6N+mWCZExNfaUbek5Z8JtaQlFhE3R8SLEbFSu2NR35CZT2Tm6pk5f1HlImJsRPyuSl3dXSMzj83M06pct4e6Fv7RIGnFZUItaYlExHDgg0ACh9RUx4A6rttbtavVtoreGvOK9rMjqXcwoZa0pD4B3AmcD3yyc2dE7BwRzzYmWhFxaETcU673i4iTI+JPETE7Ii6LiHXKY8MjIiPi0xHxBHBTuf9n5TVfjohbI2LrhmuvGxHXRMQrETEpIr7Z2CoZEVtGxA0R8UJEPBwRH+3pBUXEpyLiwYh4NSIei4h/6HJ8TERMLev6U0TsX+5fJyLOi4inyxb7K8v9b2shLV/fpuX6+RFxVkRMjIg5wJ4RcVBE/LGs48mIGN/l/F0j4vaIeKk8PjYidoyI5xqTyIg4LCKm9vA6zy+7PtxQvtZbIuJdzdyz7mLu5vqblNd8NSJuANZrONb5Hg9ouEePlWX/HBFHRcR7gQnALmX3kJfKsmtGxIURMTMiHo+Ir0ZEv4br3BYRp0fEC8BPe7jG+RHxzYZ4/l9ETCtf69UR8c4u79WxEfFo+b6eGRHRzevdH/gy8LGyrrsj4u8iYkqXcv/c8LOx1O+BpF4sM11cXFyaXoBpwD8COwDzgA0ajv0J2Kdh+2fAyeX65ykS8WHASsD/AJeUx4ZTtHhfCKwGrFLuPxpYoyz/fWBqw7UvLZdVga2AJ4HflcdWK7c/BQwAtgdmAVv38JoOAt4DBLA78Bdg+/LYaOBlYB+KRoihwJblsWspEri1gYHA7uX+sZ2xNNSRwKbl+vnlNT9QXnNlYA9gRLm9LfAc8JGy/MbAq8ARZT3rAiPLYw8ABzTUcwXwzz28zvPL6+xW3tP/avaedRdzN9e/A/jP8tq7lXX9pMt7PKCs6xVgi/LYkIZ6urt3FwJXlT8Lw4FHgE83lO8Aji+vvUoP1zgf+Ga5/qHytW1fxvrfwK1d3qtfAGuV934msH8P93R852sst1cCXgDe27Dvj8BhVd8DFxeX3ru0PQAXF5e+swC7UiTR65XbDwFfaDj+TeDccn0NYA7wrnL7QWCvhrJDymsNaEi23r2Iutcqy6wJ9C/P3aJL3Z2JyceA33Y5/3+AU5p8nVcCJzScd3o3ZYYAC4C1uznWXULXNaG+cDExfL+zXuBLwBU9lDsJuKhcX4fij4EhPZQ9H7i0YXt1YD6w0eLu2eJiLhPPDmC1hn0X03NC/RJwGOUfTz3du/K9fgPYqmHfPwA3N5R/oon7fz5vJtQ/Ar7X5T7MA4Y3vFe7Nhy/jPIPw25e93gaEupy31nAv5brWwMvAitVfQ9cXFx672KXD0lL4pPArzJzVrl9MQ3dPsrtv4niYcW/Ae7KzMfLY+8Crii7LLxEkWDPBzZoOP/JzpWI6B8R3ym7WLwCTC8PrQcMpkjMnuzu3LKunTrrKus7CtiwuxcVEQdExJ3l1+wvAQfyZneFjSha3rvaCHghM1/s7ppNaIyXiNgpIn5Tdmt4GTi2iRgAfgIcHBGrAx+lSMieaabezHyNojX1nTR3z94ScxfvBF7MzDkN+x7vrmBZ5mMUr/GZiLg2Irbs4brrAYO6XOtxim8Kmomrp1gXXq+8D7O7XPPZhvW/UCS+zboAOLLsJvJx4LLMfKO7eJfiPZDUC/nwhqSmRMQqFAlb/4joTDZWAtaKiO0y8+7MfCAiHgcOAI6kSLA7PQkcnZm3dXPt4eVqNuw+EhgD7E2RTK9J0dIXFF/Bd1B0H3mkLL9Rl7puycx9mnhdKwH/R9E3/KrMnFf2d+3sM/skRXeQrp4E1omItTLzpS7H5lB0Remso7uEKLtsXwycQdF94/WI+D5vJtRPUnQ9eftFMp+KiDuAQymSt7O6f6ULLbxPZRK+DvA0zd2zrjE3egZYOyJWa0iqN+7pnMy8Hri+/Ln6JvC/vPmwa6NZFK3H76Lo3tJ53acWEdei4oTi9Tb2W16NohvNUz2e0bO31ZWZd0bEXIrXc2S5NKryHkjqhWyhltSsj1C0KG8FjCyX9wK/pUhGO10MfI6ij+jPGvZPAP618wGsiBgcEWMWUd8aFF/1z6ZITr/VeSCLodcuB8ZHxKpl62ZjDL8ANo+Ij0fEwHLZsXzoratBFH8YzAQ6IuIAYN+G4z8CPhURe0XxYOXQiNiybAW+DvhhRKxd1rFbec7dwNYRMTIiVqboFrA4a1C0eL8eEaN5axJ2EbB3RHw0IgZE8UDmyIbjFwL/QtEH+4rF1HNgFA84DgJOA36fmU+yZPfsbcpvIiYDp0bEoIjYFTi4u7IRsUFEHFImsm8Ar1H8bEHRd3xYGV/ne30Zxc/OGuXPzz9RtMz35C3X6MbFFO/pyPIPqm+V92F6M6+1m7qGR/mQZIMLKf5A6sjMrsMA1vIeSGofE2pJzfokcF4W4wk/27lQJA1HxZsjTVxC8YDdTQ1dQ6B4+Opq4FcR8SrFA4o7LaK+Cym+ln+KomXyzi7Hx1G0Wj8L/Lis9w2AzHyVIik+nKLl71nguxSJ81uUZT9HkbS9SJHIXt1w/A8UD4mdTvFQ3i282br5cYrW04eA5ykevCQzHwG+AdwIPAo0M67yPwLfKO/N18t4OmN4gqIbyj9TdA+YCmzXcO4VZUxXdOly0Z2LgVPK6+xA0aVgie7ZIhxJ8Z6+UNZxYQ/l+pWv5emy7O4Urx+KEV7uB56NiM6fn+MpWv0fo7iXFwPnLiKO7q6xUGb+GvgaxTcTz1B8A3F4U6/w7Tr/aJwdEXc17P8xsE35b1d1vgeS2iAyF/fNmCT1fhHxXWDDzPzkYgsvhyLiT8A/ZOaNiyhzPjAjM7/assBWUGVXlucpRot5tGH/+fgeSMsdW6gl9UnleL3bRmE08GkW391huRQRh1H05b2p3bFooc8CkxqTaUnLLx9KlNRXrUHRzeOdFC2B/0ExVvEKJSJupujX/vHMXNDmcARExHSKh1o/0uZQJLWIXT4kSZKkCuzyIUmSJFVgQi1JkiRV0Of6UK+33no5fPjwdochSZKk5dyUKVNmZebgxZXrcwn18OHDmTx5crvDkCRJ0nKunP13sezyIUmSJFVgQi1JkiRVYEItSZIkVdDn+lBLkiT1NZlJR0cHzv/RO0UEAwYMICKW6nwTakmSpJp1dHTQr18/+vXrt9RJm+qRmSxYsICOjg4GDhy4VNewy4ckSVLNMtNkupeKCPr161fp2wMTakmSpBYwme69qr43JtSSJEkrgP79+zNy5Ei22247tt9+e26//fZ2h9SU888/n3HjxlUuUyf7UEuSJLXY8JOvXabXm/6dgxZbZpVVVmHq1KkAXH/99XzpS1/illtuWaZxrKhsoZYkSVrBvPLKK6y99toAvPbaa+y1115sv/32jBgxgquuugqAOXPmcNBBB7HddtuxzTbb8NOf/hSAKVOmsPvuu7PDDjuw33778cwzz7zt+mPHjuWzn/0se+65J+9+97u55ZZbOProo3nve9/L2LFjF5a75JJLGDFiBNtssw0nnXTSwv3nnXcem2++Obvvvju33Xbbwv0zZ87ksMMOY8cdd2THHXd8y7F2soVakiRpBfDXv/6VkSNH8vrrr/PMM89w0003AbDyyitzxRVX8I53vINZs2ax8847c8ghh/DLX/6Sd77znVx7bdGa/vLLLzNv3jyOP/54rrrqKgYPHsxPf/pTvvKVr3Duuee+rb4XX3yRm266iauvvpqDDz6Y2267jXPOOYcdd9yRqVOnsv7663PSSScxZcoU1l57bfbdd1+uvPJKdtppJ0455RSmTJnCmmuuyZ577sn73vc+AE444QS+8IUvsOuuu/LEE0+w33778eCDD7buJvbAhFqSJGkF0Njl44477uATn/gE9913H5nJl7/8ZW699Vb69evHU089xXPPPceIESM48cQTOemkk/jwhz/MBz/4Qe677z7uu+8+9tlnHwDmz5/PkCFDuq3v4IMPJiIYMWIEG2ywASNGjABg6623Zvr06Tz++OPsscceDB48GICjjjqKW2+9FeAt+z/2sY/xyCOPAHDjjTfywAMPLKzjlVde4dVXX63hbi0ZE2pJkqQVzC677MKsWbOYOXMmEydOZObMmUyZMoWBAwcyfPhwXn/9dTbffHOmTJnCxIkT+dKXvsS+++7LoYceytZbb80dd9yx2DpWWmklAPr167dwvXO7o6ODAQN6TkN7GnVjwYIF3HHHHayyyipL+IrrZR/qZWH8mj0vkiRJvcxDDz3E/PnzWXfddXn55ZdZf/31GThwIL/5zW94/PHHAXj66adZddVV+fu//3tOPPFE7rrrLrbYYgtmzpy5MKGeN28e999//1LFsNNOO3HLLbcwa9Ys5s+fzyWXXMLuu+/OTjvtxM0338zs2bOZN28eP/vZzxaes++++3LGGWcs3O5scW83W6iXQE9P5E5fucWBSJIkLaHOPtRQTDRzwQUX0L9/f4466igOPvhgRo0axciRI9lyyy0BuPfee/niF79Iv379GDhwIGeddRaDBg3i5z//OZ/73Od4+eWX6ejo4POf/zxbb731EsczZMgQvv3tb7PnnnuSmRx44IGMGTMGgPHjx7PLLrswZMgQtt9+e+bPnw/AD37wA4477ji23XZbOjo62G233ZgwYcIyukNLL/ranPKjRo3KyZMnt6XunhPqI3s+afzLNUUjSZL6irlz5zJo0KB2h6FF6O49iogpmTlqcefa5UOSJEmqwC4fkiStaBb1jI/frEpLzBZqSZIkqYJaW6gjYn/gv4D+wDmZ+Z0ux9cEfgJsXMby75l5Xp0xqUa2eEiSpBVQbS3UEdEfOBM4ANgKOCIitupS7DjggczcDtgD+I+IsMe+JEmS+ow6W6hHA9My8zGAiLgUGAM80FAmgTWiGL17deAFoKPGmCRJWmE43KvUGnX2oR4KPNmwPaPc1+gM4L3A08C9wAmZuaDGmCRJklY4s2fPZuTIkYwcOZINN9yQoUOHLtyeO3fuEl3r8ssv56GHHlqiczo6OlhrrbWW6JylMWzYMF566aXKZZZUnS3U3c0Z2XXQ6/2AqcCHgPcAN0TEbzPzlbdcKOIY4BiAjTfeuIZQJUmSWmhZz6a8mGeV1l133YWzCo4fP57VV1+dE088camquvzyy+nXr9/CCWBUb0I9A9ioYXsYRUt0o08B38lidplpEfFnYEvgD42FMvNs4GwoJnapLWJpWenpg3JFezjT+yBJvd4FF1zAmWeeydy5c3n/+9/PGWecwYIFC1hvvfU49thjue6661h11VW56qqrePjhh5k4cSK33XYb48eP58orr2TevHmMGzeOWbNmsdpqq3HOOeew+eab86c//YkjjzySBQsWsN9++3Vb97Rp0/jIRz7C6NGj+f3vf88OO+zAUUcdxamnnsqsWbO4+OKLGTVqFLNmzeLoo49m+vTprL766px99tlss802zJw5kyOPPJLZs2ez00470ThhYXevq1+/ejpn1NnlYxKwWURsUj5oeDhwdZcyTwB7AUTEBsAWwGM1xiRJkqTSfffdxxVXXMHtt9/O1KlT6ejo4NJLLwXg5ZdfZvfdd+fuu+9ml1124dxzz+WDH/wgBx54IKeffjpTp05l+PDhHHPMMfzwhz9kypQpfPvb32bcuHEAHH/88ZxwwglMmjSJwYMH9xjDww8/zIknnsi9Y+dxz68v4+enfJTbD5vNt3d5je8cvScAX/va19hpp5245557GD9+PGPHjgXglFNOYc899+Suu+5i//335+mnn17s66pDbS3UmdkREeOA6ymGzTs3M++PiGPL4xOA04DzI+Jeii4iJ2XmrLpikiRJ0ptuvPFGJk2axKhRxezaf/3rX9loo6KDwSqrrMIBBxwAwA477MBvf/vbt53/0ksvceedd3LYYYct3NfRUYwvcccdd3DNNdcA8PGPf5xTTjml2xg23XRTttpqK7gu2GqDQey92WoAjBiyEt++6QUAfve733HttcVDtvvuuy9jx45lzpw53HrrrUycOBGAMWPGsMYaayz2ddWh1nGoM3MiMLHLvgkN608D+9YZgyRJkrqXmRx99NGcdtppb9nf0dHBoEFvjmTcv3//hYly1/PXW2+9hf2zG0UExUBui7bSSistXO8XsNKAWLjeseDNerrW21hPs6+rLs6UKEmStILae++9ueyyy5g1q+ggMHv2bJ544olFnrPGGmvw6quvArD22mszZMgQrrjiCgAWLFjA3XffDcDOO+/MZZddBsBFF11UKc7ddttt4TVuvPFGhg0bxmqrrfaW/ddcc83CuJbmdVVhQi1JkrSCGjFiBKeccgp777032267Lfvuuy/PPffcIs854ogj+Na3vsXIkSOZPn06l156KRMmTGC77bZj66235he/+AUAP/jBDzj99NMZPXo0r732WqU4v/GNb3D77bez7bbb8vWvf53zzism1j711FO58cYb2X777bn55psZOnToUr+uKqJrE3pvN2rUqJw8eXJb6u55gPwjez5pRRrNwKnH3+ToFgXvg9RW/t7qPebOnfuWLhTqxn9s0f3+f364JdV39x5FxJTMHLW4c22hliRJkiowoZYkSZIqMKGWJEmSKqh12DxJkiQVMrP7YeTa3HdYbx+Wb0mZUC8vfCBQkhavN3xW9oYY1HIRwYIFC+jXr19TYzOrdTKTBQsWVHpfTKglSZJqNmDAADo6Opg/f343R3sY/WPu3Fpj6n3adx8iggEDlj4tNqGWJEmqWUQwcODA7g++2sOEIyvaMHt9+D74UKIkSZJUgS3UWv44mYgkSWohW6glSZKkCkyoJUmSpArs8iEtpeEnX9vjsekrtzAQSZLUVrZQS5IkSRWYUEuSJEkV2OVDkiSpZnYTXL7ZQi1JkiRVYAu1JKk1ehojHhwnXlKfZkItLa+c4EaSpJawy4ckSZJUgQm1JEmSVIEJtSRJklSBCbUkSZJUgQ8lSpKWGcfalbQiMqGWJEmt5zCKWo7Y5UOSJEmqwBZqSZKkFZ1zF1RSawt1ROwfEQ9HxLSIOLmb41+MiKnlcl9EzI+IdeqMSZIkSVqWakuoI6I/cCZwALAVcEREbNVYJjP/LTNHZuZI4EvALZn5Ql0xSZIkSctanV0+RgPTMvMxgIi4FBgDPNBD+SOAS2qMR8sRRxKQJEm9RZ1dPoYCTzZszyj3vU1ErArsD/xfD8ePiYjJETF55syZyzxQSZIkaWnV2UId3ezLHsoeDNzWU3ePzDwbOBtg1KhRPV1DkiSpeQ7dp2WkzhbqGcBGDdvDgKd7KHs4dveQJElSH1RnC/UkYLOI2AR4iiJpPrJroYhYE9gd+PsaY9Ey1FP/ZfsuS5KkFVFtCXVmdkTEOOB6oD9wbmbeHxHHlscnlEUPBX6VmXPqikWSJEmqS60Tu2TmRGBil30TumyfD5xfZxySJElSXZwpUdLyzdm/JEk1q3WmREmSJGl5Z0ItSZIkVWBCLUmSJFVgH2pJWhE4gYUk1cYWakmSJKkCE2pJkiSpAhNqSZIkqQITakmSJKkCE2pJkiSpAhNqSZIkqQITakmSJKkCE2pJkiSpAhNqSZIkqQJnSpQkqQbDT7622/3TV25xIJJqZ0ItSZKWa/5xo7qZUEt9WE+/JMBfFJIktYp9qCVJkqQKTKglSZKkCkyoJUmSpApMqCVJkqQKTKglSZKkChzlQ5IkSS2xvI5OZQu1JEmSVIEJtSRJklSBCbUkSZJUgQm1JEmSVIEPJUqSljs9PfjUlx96ktR7mVBLqmR5fWJbkqRm1drlIyL2j4iHI2JaRJzcQ5k9ImJqRNwfEbfUGY8kSZK0rNXWQh0R/YEzgX2AGcCkiLg6Mx9oKLMW8ENg/8x8IiLWryseSWqb8Wsu4tjLrYtDklSLOluoRwPTMvOxzJwLXAqM6VLmSODyzHwCIDOfrzEeSZIkaZmrM6EeCjzZsD2j3Ndoc2DtiLg5IqZExCe6u1BEHBMRkyNi8syZM2sKV5IkSVpydSbU0c2+7LI9ANgBOAjYD/haRGz+tpMyz87MUZk5avDgwcs+UkmSJGkp1TnKxwxgo4btYcDT3ZSZlZlzgDkRcSuwHfBIjXFJkiRJy0ydCfUkYLOI2AR4Cjicos90o6uAMyJiADAI2Ak4vcaY+jzHVpUkSepdakuoM7MjIsYB1wP9gXMz8/6IOLY8PiEzH4yIXwL3AAuAczLzvrpikiRJkpa1Wid2ycyJwMQu+yZ02f434N/qjEOSJEmqS60Tu0iSJEnLOxNqSZIkqQITakmSJKkCE2pJkiSpAhNqSZIkqQITakmSJKkCE2pJkiSpAhNqSZIkqQITakmSJKkCE2pJkiSpAhNqSZIkqQITakmSJKkCE2pJkiSpggGLKxAR44CLMvPFFsQjSZKkGgw/+doej01fuYWBLIcWm1ADGwKTIuIu4Fzg+szMesOSJC0pf1lKUnsststHZn4V2Az4ETAWeDQivhUR76k5NkmSJKnXa6oPddki/Wy5dABrAz+PiO/VGJskSZLU6zXTh/pzwCeBWcA5wBczc15E9AMeBf6l3hAlSZKk3quZPtTrAX+TmY837szMBRHx4XrCkiRJkvqGZrp8TARe6NyIiDUiYieAzHywrsAkSZKkvqCZFuqzgO0btud0s0+SJOltehp9xpFntDxppoU6GofJy8wFNJeIS5IkScu9ZhLqxyLicxExsFxOAB6rOzBJkiSpL2gmoT4WeD/wFDAD2Ak4ps6gJEmSpL5isV03MvN54PAWxCJJkiT1Oc2MQ70y8Glga2DhIwSZeXSNcUmSJEl9QjNdPn4MbAjsB9wCDANerTMoSZIkqa9oJqHeNDO/BszJzAuAg4AR9YYlSZIk9Q3NJNTzyn9fiohtgDWB4c1cPCL2j4iHI2JaRJzczfE9IuLliJhaLl9vOnJJkiSpF2hmPOmzI2Jt4KvA1cDqwNcWd1JE9AfOBPahGB1kUkRcnZkPdCn628x0CnNJkiT1SYtMqCOiH/BKZr4I3Aq8ewmuPRqYlpmPlde6FBgDdE2oJUmSpD5rkV0+ylkRxy3ltYcCTzZszyj3dbVLRNwdEddFxNZLWZckSZLUFs30ob4hIk6MiI0iYp3OpYnzopt92WX7LuBdmbkd8N/Ald1eKOKYiJgcEZNnzpzZRNWSJElSazSTUB8NHEfR5WNKuUxu4rwZwEYN28OApxsLZOYrmflauT4RGBgR63W9UGaenZmjMnPU4MGDm6hakiRJao1mZkrcZCmvPQnYLCI2oZi2/HDgyMYCEbEh8FxmZkSMpkjwZy9lfZIkSVLLNTNT4ie625+ZFy7qvMzsiIhxwPVAf+DczLw/Io4tj08A/hb4bER0AH8FDs/Mrt1CJEmSpF6rmWHzdmxYXxnYi6Lv8yITaljYjWNil30TGtbPAM5oKlJJkiSpF2qmy8fxjdsRsSbFdOSSJEnSCq+ZhxK7+guw2bIORJIkSeqLmulDfQ1vDnfXD9gKuKzOoCRJkqS+opk+1P/esN4BPJ6ZM2qKR5IkSepTmkmonwCeyczXASJilYgYnpnTa41MkiRJ6gOa6UP9M2BBw/b8cp8kSZK0wmsmoR6QmXM7N8r1QfWFJEmSJPUdzSTUMyPikM6NiBgDzKovJEmSJKnvaKYP9bHARRHROQHLDKDb2RMlSZKkFU0zE7v8Cdg5IlYHIjNfrT8sSZIkqW9YbJePiPhWRKyVma9l5qsRsXZEfLMVwUmSJEm9XTN9qA/IzJc6NzLzReDA+kKSJEmS+o5mEur+EbFS50ZErAKstIjykiRJ0gqjmYcSfwL8OiLOo5iC/GjgglqjkqQlMPzka3s8Nn3lFgYiSVohNfNQ4vci4l5gLyCA0zLz+tojkyRJkvqAZlqoyczrgOtqjkWSJEnqc5oZ5WPniJgUEa9FxNyImB8Rr7QiOEmSJKm3a+ahxDOAI4BHgVWAzwD/XWdQkiRJUl/RbJePaRHRPzPnA+dFxO01xyVJkiT1Cc0k1H+JiEHA1Ij4HvAMsFq9YUmSJEl9QzNdPj5elhsHzAE2Ag6rMyhJkiSpr2hm2LzHy9XXgVPrDUeSJEnqW5ppoZYkSZLUAxNqSZIkqYKmRvkAiIjVMnNOncFIUl/l9OeStOJqZmKX90fEA8CD5fZ2EfHD2iOTJEmS+oBmunycDuwHzAbIzLuB3eoMSpIkSeormupDnZlPdtk1v4ZYJEmSpD6nmT7UT0bE+4EsJ3j5HGX3D0mSJGlF10wL9bHAccBQYAYwstxerIjYPyIejohpEXHyIsrtGBHzI+Jvm7muJEmS1Fs0M7HLLOCoJb1wRPQHzgT2oUjEJ0XE1Zn5QDflvgtcv6R1SJIkSe222IQ6In7Qze6XgcmZedUiTh0NTMvMx8rrXAqMAR7oUu544P+AHZuKWJIkSepFmunysTJFN49Hy2VbYB3g0xHx/UWcNxRofJhxRrlvoYgYChwKTFhUABFxTERMjojJM2fObCJkSZIkqTWaeShxU+BDmdkBEBFnAb+i6Mpx7yLOi272ZZft7wMnZeb8iO6Klydlng2cDTBq1Kiu15AkSZLappmEeiiwGkU3D8r1d5ZJ8BuLOG8GsFHD9jDg6S5lRgGXlsn0esCBEdGRmVc2E7wkSZLUbs0k1N8DpkbEzRStzrsB34qI1YAbF3HeJGCziNgEeAo4HDiysUBmbtK5HhHnA78wmZbseAcQAAAU50lEQVQkSVJf0swoHz+KiIkUDxkG8OXM7Gxp/uIizuuIiHEUo3f0B87NzPsj4tjy+CL7TUuSJEl9QTMt1ACvA89QPKC4aURsmpm3Lu6kzJwITOyyr9tEOjPHNhmLJEmS1Gs0M2zeZ4ATKPpATwV2Bu4APlRvaJIkSVLv18yweSdQjBH9eGbuCbwPcOw6SZIkieYS6tcz83WAiFgpMx8Ctqg3LEmSJKlvaKYP9YyIWAu4ErghIl7k7cPfSZIkSSukZkb5OLRcHR8RvwHWBH5Za1SSJElSH7HIhDoi+gH3ZOY2AJl5S0uikiRJkvqIRfahzswFwN0RsXGL4pEkSZL6lGb6UA8B7o+IPwBzOndm5iG1RSVJkiT1Ec0k1KfWHoUkSZLURzXzUOItEfEuYLPMvDEiVqWYSlySJEla4S12HOqI+H/Az4H/KXcNpRhCT5IkSVrhNTOxy3HAB4BXADLzUWD9OoOSJEmS+opmEuo3MnNu50ZEDACyvpAkSZKkvqOZhPqWiPgysEpE7AP8DLim3rAkSZKkvqGZhPpkYCZwL/APwETgq3UGJUmSJPUVzQybNwa4MDP/t+5gJEmSpL6mmRbqQ4BHIuLHEXFQ2YdakiRJEk0k1Jn5KWBTir7TRwJ/iohz6g5MkiRJ6guaam3OzHkRcR3F6B6rUHQD+UydgUmSJEl9QTMTu+wfEecD04C/Bc4BhtQclyRJktQnNNNCPRa4FPiHzHyj3nAkSZKkvmWxCXVmHt64HREfAI7MzONqi0qSJEnqI5rqQx0RIykeSPwo8Gfg8jqDkiRJkvqKHhPqiNgcOBw4ApgN/BSIzNyzRbFJkiRJvd6iWqgfAn4LHJyZ0wAi4gstiUqSJEnqIxY1ysdhwLPAbyLifyNiLyBaE5YkSZLUN/SYUGfmFZn5MWBL4GbgC8AGEXFWROzbovgkSZKkXq2ZmRLnZOZFmflhYBgwFTi59sgkSZKkPmCxCXWjzHwhM/8nMz9UV0CSJElSX7JECfWSKmdZfDgipkXE21q1I2JMRNwTEVMjYnJE7FpnPJIkSdKy1tQ41EsjIvoDZwL7ADOASRFxdWY+0FDs18DVmZkRsS1wGUWfbUmSJKlPqLOFejQwLTMfy8y5FNOXj2kskJmvZWaWm6sBiSRJktSH1JlQDwWebNieUe57i4g4NCIeAq4Fju7uQhFxTNklZPLMmTNrCVaSJElaGnUm1N2NWf22FuhyeL4tgY8Ap3V3ocw8OzNHZeaowYMHL+MwJUmSpKVXZ0I9A9ioYXsY8HRPhTPzVuA9EbFejTFJkiRJy1SdCfUkYLOI2CQiBgGHA1c3FoiITSMiyvXtgUHA7BpjkiRJkpap2kb5yMyOiBgHXA/0B87NzPsj4tjy+ASK6c0/ERHzgL8CH2t4SFGSJEnq9WpLqAEycyIwscu+CQ3r3wW+W2cMkiRJUp1qndhFkiRJWt6ZUEuSJEkVmFBLkiRJFZhQS5IkSRWYUEuSJEkVmFBLkiRJFZhQS5IkSRWYUEuSJEkVmFBLkiRJFZhQS5IkSRWYUEuSJEkVmFBLkiRJFZhQS5IkSRWYUEuSJEkVmFBLkiRJFZhQS5IkSRWYUEuSJEkVmFBLkiRJFZhQS5IkSRWYUEuSJEkVmFBLkiRJFZhQS5IkSRWYUEuSJEkVmFBLkiRJFZhQS5IkSRWYUEuSJEkVmFBLkiRJFZhQS5IkSRXUmlBHxP4R8XBETIuIk7s5flRE3FMut0fEdnXGI0mSJC1rtSXUEdEfOBM4ANgKOCIitupS7M/A7pm5LXAacHZd8UiSJEl1qLOFejQwLTMfy8y5wKXAmMYCmXl7Zr5Ybt4JDKsxHkmSJGmZqzOhHgo82bA9o9zXk08D19UYjyRJkrTMDajx2tHNvuy2YMSeFAn1rj0cPwY4BmDjjTdeVvFJkiRJldXZQj0D2KhhexjwdNdCEbEtcA4wJjNnd3ehzDw7M0dl5qjBgwfXEqwkSZK0NOpMqCcBm0XEJhExCDgcuLqxQERsDFwOfDwzH6kxFkmSJKkWtXX5yMyOiBgHXA/0B87NzPsj4tjy+ATg68C6wA8jAqAjM0fVFZMkSZK0rNXZh5rMnAhM7LJvQsP6Z4DP1BmDJEmSVCdnSpQkSZIqMKGWJEmSKjChliRJkiowoZYkSZIqMKGWJEmSKjChliRJkiowoZYkSZIqMKGWJEmSKjChliRJkiowoZYkSZIqMKGWJEmSKjChliRJkiowoZYkSZIqMKGWJEmSKjChliRJkiowoZYkSZIqMKGWJEmSKjChliRJkiowoZYkSZIqMKGWJEmSKjChliRJkiowoZYkSZIqMKGWJEmSKjChliRJkiowoZYkSZIqMKGWJEmSKjChliRJkiowoZYkSZIqMKGWJEmSKqg1oY6I/SPi4YiYFhEnd3N8y4i4IyLeiIgT64xFkiRJqsOAui4cEf2BM4F9gBnApIi4OjMfaCj2AvA54CN1xSFJkiTVqc4W6tHAtMx8LDPnApcCYxoLZObzmTkJmFdjHJIkSVJt6kyohwJPNmzPKPctsYg4JiImR8TkmTNnLpPgJEmSpGWhzoQ6utmXS3OhzDw7M0dl5qjBgwdXDEuSJEladupMqGcAGzVsDwOerrE+SZIkqeXqTKgnAZtFxCYRMQg4HLi6xvokSZKklqttlI/M7IiIccD1QH/g3My8PyKOLY9PiIgNgcnAO4AFEfF5YKvMfKWuuCRJkqRlqbaEGiAzJwITu+yb0LD+LEVXEEmSJKlPcqZESZIkqQITakmSJKkCE2pJkiSpAhNqSZIkqQITakmSJKkCE2pJkiSpAhNqSZIkqQITakmSJKkCE2pJkiSpAhNqSZIkqQITakmSJKkCE2pJkiSpAhNqSZIkqQITakmSJKkCE2pJkiSpAhNqSZIkqQITakmSJKkCE2pJkiSpAhNqSZIkqQITakmSJKkCE2pJkiSpAhNqSZIkqQITakmSJKkCE2pJkiSpAhNqSZIkqQITakmSJKkCE2pJkiSpAhNqSZIkqYJaE+qI2D8iHo6IaRFxcjfHIyJ+UB6/JyK2rzMeSZIkaVmrLaGOiP7AmcABwFbAERGxVZdiBwCblcsxwFl1xSNJkiTVoc4W6tHAtMx8LDPnApcCY7qUGQNcmIU7gbUiYkiNMUmSJEnLVJ0J9VDgyYbtGeW+JS0jSZIk9VqRmfVcOOLvgP0y8zPl9seB0Zl5fEOZa4FvZ+bvyu1fA/+SmVO6XOsYii4hAFsAD9cS9NJbD5hlDL0iBugdcRiDMRiDMRiDMTSrN8RhDN17V2YOXlyhATUGMAPYqGF7GPD0UpQhM88Gzl7WAS4rETE5M0cZQ/tj6C1xGIMxGIMxGIMx9KU4jKGaOrt8TAI2i4hNImIQcDhwdZcyVwOfKEf72Bl4OTOfqTEmSZIkaZmqrYU6MzsiYhxwPdAfODcz74+IY8vjE4CJwIHANOAvwKfqikeSJEmqQ51dPsjMiRRJc+O+CQ3rCRxXZwwt0hu6oxjDm3pDHMZQMIaCMRSMoWAMBWN4U2+IwxgqqO2hREmSJGlF4NTjkiRJUgUm1BVExLkR8XxE3NfGGDaKiN9ExIMRcX9EnNCGGFaOiD9ExN1lDKe2OoaGWPpHxB8j4hdtqn96RNwbEVMjYnKbYlgrIn4eEQ+VPxe7tLj+LcrX37m8EhGfb2UMZRxfKH8e74uISyJi5TbEcEJZ//2tvAfdfTZFxDoRcUNEPFr+u3YbYvi78l4siIjan+TvIYZ/K/9v3BMRV0TEWm2I4bSy/qkR8auIeGerY2g4dmJEZESs1+oYImJ8RDzV8FlxYKtjKPcfHxEPlz+b32t1DBHx04Z7MD0iprYhhpERcWfn766IGN2GGLaLiDvK36HXRMQ76oxhmctMl6VcgN2A7YH72hjDEGD7cn0N4BFgqxbHEMDq5fpA4PfAzm26H/8EXAz8ok31TwfWa9fPQxnDBcBnyvVBwFptjKU/8CzFOJ6trHco8GdglXL7MmBsi2PYBrgPWJXieZUbgc1aVPfbPpuA7wEnl+snA99tQwzvpZhL4GZgVJvuw77AgHL9u226D+9oWP8cMKHVMZT7N6IYOODxuj+3ergP44ET6/45WEwMe5b/N1cqt9dvx3vRcPw/gK+34T78CjigXD8QuLkNMUwCdi/XjwZOa9XPxrJYbKGuIDNvBV5ocwzPZOZd5fqrwIO0eLbJLLxWbg4sl5Z3zo+IYcBBwDmtrru3KP+i3w34EUBmzs3Ml9oY0l7AnzLz8TbUPQBYJSIGUCS1bxvjvmbvBe7MzL9kZgdwC3BoKyru4bNpDMUfW5T/fqTVMWTmg5nZsom5eojhV+X7AXAnxfwHrY7hlYbN1aj583IRv6tOB/6l7voXE0PL9BDDZ4HvZOYbZZnn2xADABERwEeBS9oQQwKdLcJrUvPnZQ8xbAHcWq7fABxWZwzLmgn1ciQihgPvo2ghbnXd/cuvqZ4HbsjMlscAfJ/il8OCNtTdKYFfRcSUKGb4bLV3AzOB88quL+dExGptiKPT4dT8y6E7mfkU8O/AE8AzFGPc/6rFYdwH7BYR60bEqhStPhst5pw6bZDlOP/lv+u3MZbe4mjgunZUHBH/GhFPAkcBX29D/YcAT2Xm3a2uu4txZfeXc+vuhtSDzYEPRsTvI+KWiNixDTF0+iDwXGY+2oa6Pw/8W/kz+e/Al9oQw33AIeX639Hez8slZkK9nIiI1YH/Az7fpfWjJTJzfmaOpGjtGR0R27Sy/oj4MPB8dpm2vg0+kJnbAwcAx0XEbi2ufwDF12hnZeb7gDkUX++3XBQTOh0C/KwNda9N0SK7CfBOYLWI+PtWxpCZD1J0KbgB+CVwN9CxyJPUMhHxFYr346J21J+ZX8nMjcr6x7Wy7vIPvK/QhkS+i7OA9wAjKf7w/Y82xDAAWBvYGfgicFnZUtwOR9CGBojSZ4EvlD+TX6D8lrPFjqb4vTmFogvr3DbEsNRMqJcDETGQIpm+KDMvb2csZfeCm4H9W1z1B4BDImI6cCnwoYj4SYtjIDOfLv99HrgCqPXBjm7MAGY0fEPwc4oEux0OAO7KzOfaUPfewJ8zc2ZmzgMuB97f6iAy80eZuX1m7kbx9WY7Wp46PRcRQwDKf2v9ars3i4hPAh8Gjsqyw2YbXUzrv9p+D8Ufm3eXn5nDgLsiYsNWBpGZz5WNMQuA/6X1n5dQfGZeXnZd/APFN5y1PqDZnbJr2t8AP2113aVPUnxOQtEI0vL3IjMfysx9M3MHij8s/tTqGKowoe7jyr+kfwQ8mJn/2aYYBnc+KR8Rq1AkMw+1MobM/FJmDsvM4RTdDG7KzJa2SEbEahGxRuc6xcNPLR0BJjOfBZ6MiC3KXXsBD7QyhgbtbG15Atg5IlYt/4/sRfF8QUtFxPrlvxtT/LJs1/0AuJrilyblv1e1MZa2iYj9gZOAQzLzL22KYbOGzUNo/eflvZm5fmYOLz8zZ1A83P5sK+Po/AOvdCgt/rwsXQl8qIxnc4oHuWe1IY69gYcyc0Yb6oaiz/Tu5fqHaMMf/w2fl/2ArwITFn1GL9PupyL78kLxy/EZYB7FB9Kn2xDDrhT9du8BppbLgS2OYVvgj2UM91HzE8pNxLMHbRjlg6L/8t3lcj/wlTa9/pHA5PL9uBJYuw0xrArMBtZs48/BqRSJyn3Ajymf4m9xDL+l+IPmbmCvFtb7ts8mYF3g1xS/KH8NrNOGGA4t198AngOub0MM04AnGz4v6x5ho7sY/q/8ubwHuAYY2uoYuhyfTv2jfHR3H34M3Fveh6uBIW2IYRDwk/L9uAv4UDveC+B84Ng6617MfdgVmFJ+Vv0e2KENMZxAMVLZI8B3KCcf7CuLMyVKkiRJFdjlQ5IkSarAhFqSJEmqwIRakiRJqsCEWpIkSarAhFqSJEmqwIRakmoQEfMjYmpE3B8Rd0fEP5XjqxIRoyLiB4s4d3hEHLkUdb7lvMXVs4TX/vKyuI4kLY8cNk+SahARr2Xm6uX6+hQz4t2Wmac0ce4ewImZ+eElqG8AxViyS3TeElx/4euRJL2VCbUk1aBrAhoR7wYmUUxrvDtl4hsRuwP/VRZLYDfgBuC9wJ+BC4CzymUU0AH8U2b+JiLGAgcBKwOrUUyo03jeHxvqWQc4l2ICor8Ax2TmPRExHti43L8x8P3MfEurdkR8B/gixSQc9wOPAbMy87/K4/9KMVHLPcA3KCb12QK4FfjHzFwQEftSTLazEsWUwp/KzNeW+gZLUi8yoN0BSNKKIDMfK7t8rN/l0InAcZl5W0SsDrwOnExDS3NE/HN5jRERsSXwq3KaZIBdgG0z84WuLdvldqdTgT9m5kci4kPAhRSzagJsCewJrAE8HBFnZea8hthPjohxmTmyvO5w4HLgv8rXdDgwGhhR/rsV8DjwS+BvIuJmiqmE987MORFxEvBPFMm3JPV5JtSS1DrRzb7bgP+MiIuAyzNzRsTbiu0K/DdAZj4UEY8DnQn1DZn5QhN17wocVl7jpohYNyLWLI9dm5lvAG9ExPPABhTTAXcrM6dHxOyIeF9Z9o+ZObuM+w+Z+RhARFxS1vs6RZJ9W1lmEHBHEzFLUp9gQi1JLVB2+ZgPPE/RLQOAzPxORFwLHAjcGRF7d3f6Ii49p9kQutnX2efvjYZ982nud8M5wFhgQ4quJF2v2bgdFIn/EU1FKkl9jKN8SFLNImIwMAE4I7s8uBIR78nMezPzu8Bkiu4Xr1J0v+h0K3BUWX5zir7OD3dTVdfzGjVeYw+KPtCvLMHLmBcRAxu2rwD2B3YErm/YPzoiNim7gnwM+B1wJ/CBiNi0rH/Vhi4rktTn2UItSfVYJSKmAgMpHiT8MfCf3ZT7fETsSdEy/ABwHbAA6IiIu4HzgR8CEyLi3vJaYzPzjW66htzT5bw/NhwbD5wXEfdQPJT4ySV8PWcD90TEXZl5VGbOjYjfAC9l5vyGcncA36HoT30rcEX5UOJY4JKIWKks91XgkSWMQZJ6JUf5kCQtsbIF+i7g7zLz0XLfHtQ0bJ8k9WZ2+ZAkLZGI2AqYBvy6M5mWpBWZLdSSJElSBbZQS5IkSRWYUEuSJEkVmFBLkiRJFZhQS5IkSRWYUEuSJEkVmFBLkiRJFfx/vJ6stRJUQ5wAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "types = np.arange(1,20)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "x_axis = np.arange(len(types))\n",
    "plt.bar(x_axis -0.1, list1, width=0.2, label = 'Base model')\n",
    "plt.bar(x_axis +0.1, list2, width=0.2, label = 'Tented model')\n",
    "\n",
    "plt.xticks(x_axis, types)\n",
    "plt.title(\"Average accuracy per distortion type\")\n",
    "plt.legend(loc = 'upper right', framealpha = 0.1)\n",
    "\n",
    "plt.xlabel(\"Distortion type\")\n",
    "plt.ylabel(\"Average accuracy\")\n",
    "plt.savefig(\"cifar10c_resnet.pdf\",bbox_inches = \"tight\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31b8918c388efabf28e06d7aa829f320b99f50374aee3ccc5c6fe8cd01c3acd4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
